{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b63c2fbf",
      "metadata": {
        "id": "b63c2fbf"
      },
      "outputs": [],
      "source": [
        "# General Libraries\n",
        "import pandas as pd  # for handling dataframes\n",
        "import seaborn as sns  # for visualization\n",
        "import matplotlib.pyplot as plt  # for plotting graphs\n",
        "import numpy as np  # for numerical computing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f3c0b3e",
      "metadata": {
        "id": "0f3c0b3e"
      },
      "outputs": [],
      "source": [
        "# Scikit-Learn Libraries\n",
        "from sklearn.preprocessing import StandardScaler  # for feature scaling\n",
        "from sklearn.model_selection import train_test_split  # for splitting data\n",
        "from sklearn.metrics import mean_squared_error, confusion_matrix, recall_score, precision_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a886354b",
      "metadata": {
        "id": "a886354b"
      },
      "outputs": [],
      "source": [
        "# TensorFlow & Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, load_model  # tensorflow keras API\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Activation  # tensorflow keras layers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint  # callbacks for training\n",
        "from tensorflow.keras.optimizers import Adam  # optimizer\n",
        "from tensorflow.keras.utils import pad_sequences  # utility function for sequence padding\n",
        "from tensorflow.keras import layers # import layers explicitly"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "789bba86",
      "metadata": {
        "id": "789bba86"
      },
      "source": [
        "**Feature set description**\n",
        "\n",
        "|**Feature**         |**Description**|\n",
        "|----------------|:----------------|\n",
        "|Date Time       | year-month-day hour:minute:second   |\n",
        "|Appliances      | energy use in Wh|\n",
        "|lights          | energy use of light fixtures in the house in Wh|\n",
        "|T1              | Temperature in kitchen area, in Celsius|\n",
        "|RH_1            | Temperature in kitchen area, in Celsius|\n",
        "|T2              | Temperature in living room area, in Celsius\n",
        "|RH_2            | Humidity in living room area, in %\n",
        "|T3              | Temperature in laundry room area\n",
        "|RH_3            | Humidity in laundry room area, in %\n",
        "|T4              | Temperature in office room, in Celsius\n",
        "|RH_4            | Humidity in office room, in %\n",
        "|T5              | Temperature in bathroom, in Celsius\n",
        "|RH_5            | Humidity in bathroom, in %\n",
        "|T6              | Temperature outside the building (north side), in Celsius\n",
        "|RH_6            | Humidity outside the building (north side), in %\n",
        "|T7              | Temperature in ironing room , in Celsius\n",
        "|RH_7            | Humidity in ironing room, in %\n",
        "|T8              | Temperature in teenager room 2, in Celsius\n",
        "|RH_8            | Humidity in teenager room 2, in %\n",
        "|T9              | Temperature in parents room, in Celsius\n",
        "|RH_9            | Humidity in parents room, in %\n",
        "|To              | Temperature outside (from Chievres weather station), in Celsius\n",
        "|Pressure (from Chievres weather station) | in mm Hg\n",
        "|RH_out Humidity outside (from Chievres weather station) | in %\n",
        "|Wind speed (from Chievres weather station) | in m/s\n",
        "|Visibility (from Chievres weather station) | in km\n",
        "|Tdewpoint (from Chievres weather station) | Â°C\n",
        "|rv1 | Random variable 1 nondimensional\n",
        "|rv2 | Random variable 2 nondimensional\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b20d072",
      "metadata": {
        "id": "5b20d072"
      },
      "outputs": [],
      "source": [
        "file_path = 'energydata_complete.csv'\n",
        "\n",
        "# Read our dataset into our dataframe\n",
        "df = pd.read_csv(file_path)\n",
        "df_raw = df.copy()\n",
        "\n",
        "# Print the head of the dataframe\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d26759b2",
      "metadata": {
        "id": "d26759b2"
      },
      "outputs": [],
      "source": [
        "# Check for missing values in the features\n",
        "df.isna().sum()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf02485c",
      "metadata": {
        "id": "cf02485c"
      },
      "source": [
        "Per our output, we see not missing values in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b178094",
      "metadata": {
        "id": "7b178094"
      },
      "outputs": [],
      "source": [
        "# Conver date to Datetime format\n",
        "df['Datetime'] = pd.to_datetime(df['date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6321e5c0",
      "metadata": {
        "id": "6321e5c0"
      },
      "outputs": [],
      "source": [
        "# Verify our dataframe types\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fb670b8",
      "metadata": {
        "id": "8fb670b8"
      },
      "outputs": [],
      "source": [
        "# Round all values to 2 decimal places for easier processing and formatting\n",
        "df = df.round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0bc8919",
      "metadata": {
        "id": "b0bc8919"
      },
      "outputs": [],
      "source": [
        "# Validate our rounding\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f75725",
      "metadata": {
        "id": "72f75725"
      },
      "outputs": [],
      "source": [
        "# Drop the 'Date' column\n",
        "df = df.drop(columns=['date'])\n",
        "\n",
        "# Move the 'Datetime' column to the first position\n",
        "df = df[['Datetime'] + [col for col in df.columns if col != 'Datetime']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec48a7c",
      "metadata": {
        "id": "fec48a7c"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cfdd85d",
      "metadata": {
        "id": "2cfdd85d"
      },
      "outputs": [],
      "source": [
        "correlation_matrix = df.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d8d79f8",
      "metadata": {
        "id": "5d8d79f8"
      },
      "outputs": [],
      "source": [
        "# Set up the figure size\n",
        "plt.figure(figsize=(12, 10))\n",
        "\n",
        "# Plot the heatmap\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=\".2f\", cbar=True)\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Heatmap of Correlations Between Variables', fontsize=16)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bf73670",
      "metadata": {
        "id": "6bf73670"
      },
      "source": [
        "The below features were chosen based on their strong correlation with Appliances (energy consumption) while avoiding redundant or weak predictors. <br>\n",
        "lights was included because lighting directly contributes to household energy use. Indoor temperature features (T1, T2) were selected as they <br>\n",
        "impact HVAC operations, which are a major factor in energy consumption. T_out (outdoor temperature) and Tdewpoint were chosen because external <br>\n",
        "weather conditions influence heating and cooling requirements inside the home. RH_out (outdoor humidity) and Windspeed were kept as they can affect <br>\n",
        "temperature regulation and ventilation needs. Finally, hour was included to capture daily energy usage patterns, as appliance consumption tends to <br>\n",
        "vary throughout the day. These features provide a balanced mix of internal conditions, external environmental influences, and time-based patterns, <br>\n",
        "ensuring the model captures key factors affecting energy consumption.\n",
        "\n",
        "|**Feature**         |**Description**|\n",
        "|----------------|:----------------|\n",
        "|lights                    | Directly affects energyh consumption  |\n",
        "|T1 (Kitchen Temp)         | Strong correlation with Appliances    |\n",
        "|T2 (Living Room Temp)     | Represents HVAC energy use            |\n",
        "|T_out (Outside Temp)      | External factor on energy consumption |\n",
        "|RH_out (Outdoor Humidity) | Impcasts cooling and heating          |\n",
        "|Windspeed                 | Influences temp reulation             |\n",
        "|Tdewpoint                 | Outdoor conditions                    |\n",
        "|hour                      | Captures time-based energy patterns   |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af254f76",
      "metadata": {
        "id": "af254f76"
      },
      "outputs": [],
      "source": [
        "# Plot a distribution of the energy consumption\n",
        "sns.histplot(df['Appliances'], bins=30, kde=True)\n",
        "plt.title(\"Appliances Energy Consumption Distribution\")\n",
        "plt.xlabel(\"Energy Consumption (Wh)\")\n",
        "plt.ylabel(\"Occurrence\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54999a3a",
      "metadata": {
        "id": "54999a3a"
      },
      "source": [
        "1. We can see from our distribution plot for energy consumption that most of the consumption is < 200 Wh\n",
        "2. A sharp peak > 50 Wh < 100 Wh suggests many appliances consume low energy.\n",
        "3. We have a right skewed plot, dmeonstrating that anything > 400 Wh is rare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a683e24",
      "metadata": {
        "id": "5a683e24"
      },
      "outputs": [],
      "source": [
        "# Extract time-based features\n",
        "df['hour'] = df['Datetime'].dt.hour  # Extract hour of the day (0-23)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2d33e4c",
      "metadata": {
        "id": "f2d33e4c"
      },
      "outputs": [],
      "source": [
        "# Plot raw data (individual appliance consumption points)\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(df['hour'], df['Appliances'], alpha=0.3, color='blue', label=\"Appliance Energy Consumption\")\n",
        "\n",
        "# Overlay the 24-hour average trend\n",
        "hourly_avg = df.groupby('hour')['Appliances'].mean()\n",
        "plt.plot(hourly_avg.index, hourly_avg.values, marker='o', linestyle='-', color='red', label=\"Hourly Average\")\n",
        "\n",
        "# Plot our hourly average across the dataset\n",
        "plt.xlabel(\"Hour of the Day\")\n",
        "plt.ylabel(\"Energy Consumption (Wh)\")\n",
        "plt.title(\"Appliance Energy Consumption by Hour of the Day\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05ff3f2",
      "metadata": {
        "id": "f05ff3f2"
      },
      "source": [
        "The hourly average did not provide to strong of an insight. As expected, energy consumption is lowest during sleeping hours. <br>\n",
        "Early evening has the peak on average."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16e7a69a",
      "metadata": {
        "id": "16e7a69a"
      },
      "outputs": [],
      "source": [
        "#sns.pairplot(df[['Appliances', 'lights', 'T1', 'T2', 'T_out', 'RH_out', 'Windspeed', 'Tdewpoint', 'hour']])\n",
        "#plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b804243",
      "metadata": {
        "id": "3b804243"
      },
      "source": [
        "# Feedfoward Neural Network (FNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e42094",
      "metadata": {
        "id": "50e42094"
      },
      "outputs": [],
      "source": [
        "# set our features and target\n",
        "features = ['lights', 'T1', 'T2', 'T_out', 'RH_out', 'Windspeed', 'Tdewpoint', 'hour']\n",
        "target = 'Appliances'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fea87554",
      "metadata": {
        "id": "fea87554"
      },
      "outputs": [],
      "source": [
        "# Prepare our data for training\n",
        "X = df[features]  # independent variables\n",
        "y = df[target]  # target variable\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split dataset into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef726004",
      "metadata": {
        "id": "ef726004"
      },
      "outputs": [],
      "source": [
        "print(f\"Training Set: {X_train.shape}, Testing Set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dd79dc9",
      "metadata": {
        "id": "0dd79dc9"
      },
      "outputs": [],
      "source": [
        "# build of FNN\n",
        "model = Sequential([\n",
        "    layers.Input(shape=(X_train.shape[1],)),  # input\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # output layer for energy consumption prediction\n",
        "])\n",
        "\n",
        "# compile the model\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "# model evaluation\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f\"Neural Network MAE: {mae:.2f} Wh\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afa44659",
      "metadata": {
        "id": "afa44659"
      },
      "outputs": [],
      "source": [
        "# Extract loss and validation loss from the training history\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.title(\"Training & Validation Loss Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4f911f1",
      "metadata": {
        "id": "f4f911f1"
      },
      "source": [
        "# FNN Optimized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96272fa3",
      "metadata": {
        "id": "96272fa3"
      },
      "outputs": [],
      "source": [
        "# callbacks to prevent overfitting and optimize learning\n",
        "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=1e-5)\n",
        "\n",
        "# updated our FNN model\n",
        "model = Sequential([\n",
        "    layers.Input(shape=(X_train.shape[1],)),  # input Layer\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),  # dropout to reduce overfitting\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # output layer for regression\n",
        "])\n",
        "\n",
        "# compile the model with a lower learning rate for stable training\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
        "\n",
        "# train the model with early stopping to prevent overfitting\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=100, batch_size=16,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# evaluate the model\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f\"Optimized Neural Network MAE: {mae:.2f} Wh\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba754a1",
      "metadata": {
        "id": "5ba754a1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Extract loss and validation loss from the training history\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss (MSE)\")\n",
        "plt.title(\"Training & Validation Loss Over Epochs\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2779d0ec",
      "metadata": {
        "id": "2779d0ec"
      },
      "source": [
        "# LSTM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "feat_cols = ['Appliances']  # Define the features\n",
        "df[feat_cols] = scaler.fit_transform(df[feat_cols])\n",
        "\n",
        "# Define target variable\n",
        "target_col = 'Appliances'"
      ],
      "metadata": {
        "id": "GfOpnUpqhebw"
      },
      "id": "GfOpnUpqhebw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and validation sets\n",
        "train_size = int(0.8 * len(df))\n",
        "train_df = df.iloc[:train_size].reset_index(drop=True)\n",
        "val_df = df.iloc[train_size:].reset_index(drop=True)\n",
        "\n",
        "# Define X_train (features for LSTM)\n",
        "X_train = train_df[feat_cols].values\n",
        "\n",
        "# Define model parameters\n",
        "nb_features = X_train.shape[1]  # Now correctly defined\n",
        "nb_out = 1"
      ],
      "metadata": {
        "id": "Y5wM6l9Thf26"
      },
      "id": "Y5wM6l9Thf26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence parameters\n",
        "seq_length = 60\n",
        "ph = 5\n",
        "\n",
        "# Prepare training sequences\n",
        "seq_arrays = []\n",
        "seq_labs = []\n",
        "\n",
        "for i in range(len(train_df) - seq_length - ph):\n",
        "    seq = train_df[feat_cols].iloc[i:i + seq_length].values\n",
        "    label = train_df[target_col].iloc[i + seq_length + ph - 1]\n",
        "    seq_arrays.append(seq)\n",
        "    seq_labs.append(label)\n",
        "\n",
        "# Convert sequences to NumPy arrays\n",
        "seq_arrays = np.array(seq_arrays, dtype=np.float32)\n",
        "seq_labs = np.array(seq_labs, dtype=np.float32).reshape(-1)"
      ],
      "metadata": {
        "id": "XdRXSLjNhfxf"
      },
      "id": "XdRXSLjNhfxf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model path\n",
        "model_path = 'LSTM_base_model.keras'\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, input_shape=(seq_length, nb_features), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=25, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=nb_out))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
        "\n",
        "# Print model summary\n",
        "print(model.summary())\n",
        "\n",
        "# Train the model without callbacks\n",
        "history = model.fit(\n",
        "    seq_arrays, seq_labs,\n",
        "    epochs=100,\n",
        "    batch_size=500,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "id": "BkJ9pvojhfuK"
      },
      "id": "BkJ9pvojhfuK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare validation sequences\n",
        "val_arrays = []\n",
        "val_labs = []\n",
        "\n",
        "for i in range(seq_length, len(val_df) - ph):\n",
        "    seq = val_df[feat_cols].iloc[i - seq_length:i].values\n",
        "    label = val_df[feat_cols].iloc[i + ph - 1].values\n",
        "    val_arrays.append(seq)\n",
        "    val_labs.append(label)\n",
        "\n",
        "val_arrays = np.array(val_arrays, dtype=np.float32)\n",
        "val_labs = np.array(val_labs, dtype=np.float32).reshape(-1)\n",
        "\n",
        "# Evaluate the model\n",
        "scores_test = model.evaluate(val_arrays, val_labs, verbose=2)\n",
        "print('\\nMSE: {}'.format(scores_test[1]))\n",
        "\n",
        "# Predictions\n",
        "y_pred_test = model.predict(val_arrays)\n",
        "y_true_test = val_labs\n",
        "\n",
        "# Plot the results\n",
        "fig_verify = plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_pred_test[-500:], label='Predicted Value')\n",
        "plt.plot(y_true_test[-500:], label='Actual Value')\n",
        "plt.title('Appliances Power Prediction - Last 500 Points', fontsize=22, fontweight='bold')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('Row')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig_verify.savefig(\"model_regression_verify.png\")"
      ],
      "metadata": {
        "id": "N97RoTeDhfms"
      },
      "id": "N97RoTeDhfms",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Optimized"
      ],
      "metadata": {
        "id": "fU1mB4Kbh99_"
      },
      "id": "fU1mB4Kbh99_"
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "feat_cols = ['Appliances']\n",
        "df[feat_cols] = scaler.fit_transform(df[feat_cols])\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_size = int(0.8 * len(df))\n",
        "train_df = df.iloc[:train_size].reset_index(drop=True)  # First 80% for training\n",
        "val_df = df.iloc[train_size:].reset_index(drop=True)    # Last 20% for validation"
      ],
      "metadata": {
        "id": "SSTJkN_ghfjS"
      },
      "id": "SSTJkN_ghfjS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequence parameters\n",
        "seq_length = 60  # Increased sequence length\n",
        "ph = 5\n",
        "\n",
        "# Prepare training sequences\n",
        "seq_arrays = []\n",
        "seq_labs = []\n",
        "\n",
        "for i in range(len(train_df) - seq_length - ph):\n",
        "    seq = train_df[feat_cols].iloc[i:i + seq_length].values\n",
        "    label = train_df[feat_cols].iloc[i + seq_length + ph - 1].values\n",
        "    seq_arrays.append(seq)\n",
        "    seq_labs.append(label)\n",
        "\n",
        "seq_arrays = np.array(seq_arrays, dtype=np.float32)\n",
        "seq_labs = np.array(seq_labs, dtype=np.float32).reshape(-1)\n",
        "\n",
        "assert seq_arrays.shape == (len(train_df) - seq_length - ph, seq_length, len(feat_cols))\n",
        "assert seq_labs.shape == (len(train_df) - seq_length - ph,)"
      ],
      "metadata": {
        "id": "MEKm2jD5hfgC"
      },
      "id": "MEKm2jD5hfgC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "model_path = 'LSTM_model1.keras'\n",
        "nb_features = len(feat_cols)\n",
        "nb_out = 1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, input_shape=(seq_length, nb_features), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(units=25, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units=nb_out))\n",
        "model.add(Activation('linear'))\n",
        "\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.005)  # Adjusted learning rate\n",
        "model.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['mse'])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Callbacks\n",
        "\n",
        "# Learning rate scheduler: Reduces learning rate by a factor of 0.5 if validation loss does not improve for 5 epochs\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
        "\n",
        "# Early stopping: Stops training if validation loss does not improve for 10 consecutive epochs, preventing overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\n",
        "\n",
        "# Model checkpoint: Saves the model with the best validation loss during training to avoid saving suboptimal weights\n",
        "model_checkpoint = ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    seq_arrays, seq_labs,\n",
        "    epochs=100,\n",
        "    batch_size=500,\n",
        "    validation_split=0.1,\n",
        "    verbose=2,\n",
        "    callbacks=[lr_scheduler, early_stopping, model_checkpoint]\n",
        ")\n"
      ],
      "metadata": {
        "id": "2nmDY-gThfcs"
      },
      "id": "2nmDY-gThfcs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare validation sequences\n",
        "val_arrays = []\n",
        "val_labs = []\n",
        "\n",
        "for i in range(seq_length, len(val_df) - ph):\n",
        "    seq = val_df[feat_cols].iloc[i - seq_length:i].values\n",
        "    label = val_df[feat_cols].iloc[i + ph - 1].values\n",
        "    val_arrays.append(seq)\n",
        "    val_labs.append(label)\n",
        "\n",
        "val_arrays = np.array(val_arrays, dtype=np.float32)\n",
        "val_labs = np.array(val_labs, dtype=np.float32).reshape(-1)\n",
        "\n",
        "# Evaluate the model\n",
        "scores_test = model.evaluate(val_arrays, val_labs, verbose=2)\n",
        "print('\\nMSE: {}'.format(scores_test[1]))\n",
        "\n",
        "# Predictions\n",
        "y_pred_test = model.predict(val_arrays)\n",
        "y_true_test = val_labs\n",
        "\n",
        "# Plot the results\n",
        "fig_verify = plt.figure(figsize=(10, 5))\n",
        "plt.plot(y_pred_test[-500:], label='Predicted Value')\n",
        "plt.plot(y_true_test[-500:], label='Actual Value')\n",
        "plt.title('Appliances Power Prediction - Last 500 Points', fontsize=22, fontweight='bold')\n",
        "plt.ylabel('Value')\n",
        "plt.xlabel('Row')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig_verify.savefig(\"model_regression_verify.png\")"
      ],
      "metadata": {
        "id": "X_bulW5piUG7"
      },
      "id": "X_bulW5piUG7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}